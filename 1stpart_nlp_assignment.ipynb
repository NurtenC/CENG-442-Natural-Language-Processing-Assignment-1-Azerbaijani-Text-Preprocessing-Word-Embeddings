{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDPzvQ5xke8z",
        "outputId": "2b1b8075-66bd-4e9a-fee1-3d720eff7755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas gensim openpyxl regex ftfy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4GQi6iEQiuOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path_raw = \"/content/drive/MyDrive/Colab Notebooks/nlp/Excels/\"\n",
        "drive_path_cleaned = \"/content/drive/MyDrive/Colab Notebooks/nlp/Cleaned/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mZPG0RqsW3V",
        "outputId": "f8312230-7c0a-449c-db7c-b63a1cafc524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**START POINT**"
      ],
      "metadata": {
        "id": "z8XrpOsfeXpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, html, unicodedata\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    def fix_text(s): return s\n"
      ],
      "metadata": {
        "id": "ZIYR0PMRf6WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_az(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.replace('İ', 'i').replace('I', 'ı')\n",
        "    s = s.lower()\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    return s\n",
        "\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
        "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "REPEAT_CHARS= re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
        "\n",
        "TOKEN_RE = re.compile(\n",
        "    r\"EMO_(?:POS|NEG)|<[A-Z0-9_]+>|<NUM>|URL|EMAIL|PHONE|USER\"\n",
        "    r\"|\\d+\"\n",
        "    r\"|[A-Za-zƏəĞğIıİiÖöÜüÇçŞşXxQq]+(?:'[A-Za-zƏəĞğIıİiÖöÜüÇçŞşXxQq]+)?\"\n",
        ")\n",
        "\n",
        "\n",
        "EMO_MAP = {\n",
        "    \"🙂\":\"EMO_POS\",\"😀\":\"EMO_POS\",\"😍\":\"EMO_POS\",\"😊\":\"EMO_POS\",\"👍\":\"EMO_POS\",\n",
        "    \"😃\":\"EMO_POS\",\"😄\":\"EMO_POS\",\"😁\":\"EMO_POS\",\"😆\":\"EMO_POS\",\"😅\":\"EMO_POS\",\n",
        "    \"🤣\":\"EMO_POS\",\"😂\":\"EMO_POS\",\"🙃\":\"EMO_POS\",\"😉\":\"EMO_POS\",\n",
        "    \"☹\":\"EMO_NEG\",\"🙁\":\"EMO_NEG\",\"😠\":\"EMO_NEG\",\"😡\":\"EMO_NEG\",\"👎\":\"EMO_NEG\",\n",
        "    \"😒\":\"EMO_NEG\",\"😓\":\"EMO_NEG\",\"😔\":\"EMO_NEG\",\"😖\":\"EMO_NEG\",\"😞\":\"EMO_NEG\",\n",
        "    \"😟\":\"EMO_NEG\",\"😤\":\"EMO_NEG\",\"😢\":\"EMO_NEG\",\"😭\":\"EMO_NEG\",\"😦\":\"EMO_NEG\",\n",
        "}\n",
        "\n",
        "SLANG_MAP = {\n",
        "    \"slm\":\"salam\",\"tmm\":\"tamam\",\"sagol\":\"sağol\",\"cox\":\"çox\",\"yaxsi\":\"yaxşı\",\n",
        "    }\n",
        "\n",
        "NEGATE_PREV = {\n",
        "    \"deyil\",\"deyildi\",\"deyilim\",\"deyilsən\",\"deyildir\",\n",
        "    \"deyilik\",\"deyilsiniz\",\"deyildilər\",\"yox\",\"yoxdur\",\"yoxdu\"\n",
        "}\n",
        "\n",
        "NEGATE_NEXT = {\"heç\",\"qətiyyən\"}\n"
      ],
      "metadata": {
        "id": "VFDcp0-rf7ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
        "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:😂|😍|😊|👍|👎|😡|🙂|USER)\")\n",
        "REV_HINTS = re.compile(r\"\\b(azn|manat|qiymət|aldım|ulduz|çox yaxşı|çox pis|STARS|RATING_POS|RATING_NEG)\\b\", re.I)\n",
        "\n",
        "PRICE_RE = re.compile(r\"(?:\\b\\d+\\b|<NUM>)\\s*(?:azn|manat|₼)\\b\", re.I)\n",
        "STARS_RE = re.compile(r\"(?:(?<!<)\\b([1-5])\\b|<NUM>)\\s*[- ]*ulduz\\b\", re.I)\n",
        "POS_RATE = re.compile(r\"\\bçox yaxşı\\b\")\n",
        "NEG_RATE = re.compile(r\"\\bçox pis\\b\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oGh4cP2PhtFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_domain(text: str) -> str:\n",
        "    s = text.lower()\n",
        "    if NEWS_HINTS.search(s): return \"news\"\n",
        "    if SOCIAL_HINTS.search(s): return \"social\"\n",
        "    if REV_HINTS.search(s): return \"reviews\"\n",
        "    return \"general\"\n",
        "\n",
        "def _stars_sub(m):\n",
        "    n = m.group(1)\n",
        "    return f\" <STARS_{n}> \" if n else \" <STARS> \"\n",
        "\n",
        "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
        "    if domain == \"reviews\":\n",
        "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
        "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
        "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
        "        s = STARS_RE.sub(_stars_sub, s)\n",
        "        return \" \".join(s.split())\n",
        "    return cleaned\n",
        "\n",
        "def add_domain_tag(line: str, domain: str) -> str:\n",
        "    return f\"dom{domain} \" + line\n"
      ],
      "metadata": {
        "id": "D5nyg7-8hxmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "\n",
        "    s = fix_text(s)\n",
        "    s = html.unescape(s)\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)).lower() + \" \", s)\n",
        "\n",
        "    s = lower_az(s)\n",
        "\n",
        "    for emo, tag in EMO_MAP.items():\n",
        "        s = s.replace(emo, f\" {tag} \")\n",
        "\n",
        "    s = URL_RE.sub(\" URL \", s)\n",
        "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
        "    s = PHONE_RE.sub(\" PHONE \", s)\n",
        "    s = USER_RE.sub(\" USER \", s)\n",
        "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
        "    if numbers_to_token:\n",
        "        s = re.sub(r\"(?i)(?<!<)\\d+(?!\\s*[- ]*ulduz\\b)\", \" <NUM> \", s)\n",
        "    if keep_sentence_punct:\n",
        "        s = re.sub(r\"[^\\w\\s<>'_əğıöşüçƏĞIİÖŞÜÇxqXQ.!?]\", \" \", s)\n",
        "    else:\n",
        "        s = re.sub(r\"[^\\w\\s<>'_əğıöşüçƏĞIİÖŞÜÇxqXQ]\", \" \", s)\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "\n",
        "    toks = TOKEN_RE.findall(s)\n",
        "    norm = []\n",
        "    mark_neg = 0\n",
        "\n",
        "    for t in toks:\n",
        "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
        "        t = SLANG_MAP.get(t, t)\n",
        "\n",
        "        if mark_neg > 0 and t not in {\"URL\",\"EMAIL\",\"PHONE\",\"USER\"} and not t.startswith(\"<\"):\n",
        "            norm.append(t + \"_NEG\")\n",
        "            mark_neg -= 1\n",
        "            continue\n",
        "\n",
        "        if t in NEGATE_PREV:\n",
        "            i = len(norm) - 1\n",
        "            while i >= 0:\n",
        "                if not norm[i].endswith(\"_NEG\") and norm[i] not in {\"URL\",\"EMAIL\",\"PHONE\",\"USER\"} and not norm[i].startswith(\"<\"):\n",
        "                    norm[i] = norm[i] + \"_NEG\"\n",
        "                    break\n",
        "                i -= 1\n",
        "            norm.append(t)\n",
        "            continue\n",
        "\n",
        "        if t in NEGATE_NEXT:\n",
        "            norm.append(t)\n",
        "            mark_neg = 3\n",
        "            continue\n",
        "\n",
        "        norm.append(t)\n",
        "\n",
        "    norm = [t for t in norm if len(t) > 1 or t.isdigit() or t in {\"o\", \"e\"}]\n",
        "    return \" \".join(norm).strip()"
      ],
      "metadata": {
        "id": "AJhy3Jaih1gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentiment_value(v, scheme: str):\n",
        "    if scheme == \"binary\":\n",
        "        try:\n",
        "            return 1.0 if int(v) == 1 else 0.0\n",
        "        except Exception:\n",
        "            return None\n",
        "    s = str(v).strip().lower()\n",
        "    if s in {\"pos\",\"positive\",\"1\",\"müsbət\",\"good\",\"pozitiv\"}: return 1.0\n",
        "    if s in {\"neu\",\"neutral\",\"2\",\"neytral\"}: return 0.5\n",
        "    if s in {\"neg\",\"negative\",\"0\",\"mənfi\",\"bad\",\"neqativ\"}: return 0.0\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "uyPivalVh7Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
        "    df = pd.read_excel(in_path)\n",
        "    for c in [\"Unnamed: 0\",\"index\"]:\n",
        "        if c in df.columns: df = df.drop(columns=[c])\n",
        "    assert text_col in df.columns and label_col in df.columns, f\"Missing columns in {in_path}\"\n",
        "\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "\n",
        "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s, numbers_to_token=False))# false?\n",
        "\n",
        "    df[\"__domain__\"] = df[text_col].astype(str).apply(detect_domain)\n",
        "    df[\"cleaned_text\"] = df.apply(lambda r: domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
        "\n",
        "    df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: re.sub(r\"(?<!<STARS_)\\b\\d+\\b\", \" <NUM> \", s))\n",
        "    df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: MULTI_SPACE.sub(\" \", s).strip())\n",
        "\n",
        "\n",
        "    if remove_stopwords:\n",
        "        sw = set([\"və\",\"ilə\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"həm\",\"ki\",\"bu\",\"bir\",\"o\",\"biz\",\"siz\",\"mən\",\"sən\",\n",
        "                  \"orada\",\"burada\",\"bütün\",\"hər\",\"artıq\",\"çox\",\"az\",\"ən\",\"də\",\"da\",\"üçün\", \"necə\", \"şey\", \"isə\",\n",
        "                  \"hələ\", \"nə\", \"niyə\", \"kimi\", \"belə\", \"indi\", \"qədər\"])\n",
        "        for keep in set(list(NEGATE_PREV) + list(NEGATE_NEXT)):\n",
        "            sw.discard(keep)\n",
        "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: \" \".join([t for t in s.split() if t not in sw]))\n",
        "\n",
        "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
        "    df = df.dropna(subset=[\"sentiment_value\"])\n",
        "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
        "\n",
        "    out_df = df[[\"cleaned_text\",\"sentiment_value\"]].reset_index(drop=True)\n",
        "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_df.to_excel(out_two_col_path, index=False)\n",
        "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n"
      ],
      "metadata": {
        "id": "jy_s_s66iCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
        "    \"\"\"Create domain-tagged, lowercase, punctuation-free corpus (one sentence per line).\"\"\"\n",
        "    lines = []\n",
        "    for (f, text_col) in zip(input_files, text_cols):\n",
        "        df = pd.read_excel(f)\n",
        "        for raw in df[text_col].dropna().astype(str):\n",
        "            dom = detect_domain(raw)\n",
        "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
        "            parts = re.split(r\"[.!?]+\", s)\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p: continue\n",
        "                p = re.sub(r\"[^\\w\\səğıöşüçƏĞIİÖŞÜÇxqXQ]\", \" \", p)\n",
        "                p = \" \".join(p.split()).lower()\n",
        "                if p:\n",
        "                    lines.append(f\"dom{dom} \" + p)\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "        for ln in lines:\n",
        "            w.write(ln + \"\\n\")\n",
        "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n"
      ],
      "metadata": {
        "id": "6cSFy59IiC_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    CFG = [\n",
        "        (f\"{drive_path_raw}labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
        "        (f\"{drive_path_raw}test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (f\"{drive_path_raw}train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (f\"{drive_path_raw}train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
        "        (f\"{drive_path_raw}merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
        "    ]\n",
        "\n",
        "    for fname, tcol, lcol, scheme in CFG:\n",
        "        out_filename = f\"{Path(fname).stem}_2col.xlsx\"\n",
        "\n",
        "        full_out_path = f\"{drive_path_cleaned}/{out_filename}\"\n",
        "\n",
        "        process_file(fname, tcol, lcol, scheme, full_out_path, remove_stopwords=False)\n",
        "\n",
        "    corpus_output_path = f\"{drive_path_cleaned}/corpus_all.txt\"\n",
        "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt=corpus_output_path)\n",
        "    print(drive_path_cleaned)"
      ],
      "metadata": {
        "id": "ufgmLan5iIS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64176e3e-3ef9-44c9-ffb8-cbf3bfbad08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//labeled-sentiment_2col.xlsx (rows=2955)\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//test__1__2col.xlsx (rows=4198)\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//train__3__2col.xlsx (rows=19557)\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//train-00000-of-00001_2col.xlsx (rows=41756)\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
            "Wrote /content/drive/MyDrive/Colab Notebooks/nlp/Cleaned//corpus_all.txt with 124353 lines\n",
            "/content/drive/MyDrive/Colab Notebooks/nlp/Cleaned/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnnKYt106-QH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}